import argparse
import logging
import requests


def setup_logging(debug):
    '''
        Bootstraps logging
        Arguments:
            debug (boolean) increases logging verbosity
    '''
    logger = logging.getLogger()
    handler = logging.StreamHandler()
    if debug:
        handler.setFormatter(
                logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')
                )
        logger.setLevel(logging.DEBUG)
    else:
        handler.setFormatter(
                logging.Formatter('%(levelname)s:%(message)s')
                )
        logger.setLevel(logging.INFO)
    logger.addHandler(handler)


def get_random_user_agent():
    '''
        Selects a random user agent from user_agents.txt
        to be used in HTTP requests

        Returns:
            user_agent_list (string) containing a randomly
            selected user agent
    '''
    user_agent_list = []
    line_count = 0

    logging.debug('Getting random user agent')
    with open('data/user_agents/user_agents.txt', 'r') as file_handle:
        for user_agent in file_handle:
            user_agent_list.append(user_agent)
            line_count += 1


def build_http_header():
    '''
        Builds a specific HTTP header for the requested
        website

        Returns:
            requests_header (dict) containing specific HTTP header
            options set for the requested website
    '''
    requests_header = {}

    requests_header['Accept'] = 'text/html'
    requests_header['Accept-Language'] = 'en-US, en;'
    requests_header['Accept-Encoding'] = 'gzip, deflate'
    requests_header['User-Agent'] = get_random_user_agent()
    requests_header['Referer'] = 'https://google.com'

    return requests_header


def check_url_is_alive(response_code):
    '''
        Checks to see if the specified url is
        returning a HTTP header with a response
        code of 2xx or 3xx
    '''
    skip_url = None
    if (response_code == requests.codes.ok
            or response_code[0] == 3):
        logging.debug('URL responded with 2xx or 3xx')


def parse_urls(urls):
    '''
        Connects to the URLs specified in a file and
        mimics a browser intending to trigger an exploit
        attempt, payloads of the exploit are then saved

        Arguments:
            urls(filehandle) containing a list of urls
    '''
    http_header = build_http_header()
    for url in urls:
        web_request = requests.get(url.strip(), headers=http_header)
        check_url_is_alive(web_request.status_code)


def main():
    argument_parser = argparse.ArgumentParser()
    argument_parser.add_argument(
            '-u',
            '--urls',
            required=True,
            type=argparse.FileType('r'),
            help='File containing urls to parse')
    argument_parser.add_argument(
            '-d',
            '--debug',
            action='store_true',
            help='Enable debug logging')

    arguments = argument_parser.parse_args()

    setup_logging(arguments.debug)

    parse_urls(arguments.urls)


if __name__ == '__main__':
    main()
